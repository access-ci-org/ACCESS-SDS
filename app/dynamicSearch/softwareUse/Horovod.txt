Use Case: Horovod is primarily used as a distributed deep learning framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It can be used for training machine learning models across multiple GPUs in parallel.

Code details and examples:

The following snippet of code should help you get started with TensorFlow model training using Horovod.

Code:

```python
import tensorflow as tf
import horovod.tensorflow as hvd

# Initialize Horovod
hvd.init()

# Pin GPU to be used to process local rank
config = tf.ConfigProto()
config.gpu_options.visible_device_list = str(hvd.local_rank())

# Build your model...
model = Sequential()

...

# Note: if you're using a loss function like categorical_crossentropy, make sure  
# to set the `sparse` flag to False.
opt = tf.train.AdagradOptimizer(0.01 * hvd.size())

# Wrap the optimizer with Horovod's DistributedOptimizer
opt = hvd.DistributedOptimizer(opt)

# Train your model.
model.compile(loss=tf.losses.mean_squared_error,
              optimizer=opt,
              metrics=['accuracy'],
              distribute=DistributionStrategy.HOROVOD)
```

You can run the above script using the following command in your terminal:

```bash
horovodrun -np 4 -H localhost:4 python train.py
```

This command runs the training script on 4 GPUs on the local machine. The number after -np stands for the total number of tasks to be run. The -H flag is followed by a comma-separated list of <hostname>:<slots> pairs, where the <hostname> is the hostname of the machine and the <slots> is the number of GPUs on that machine. 

Input Files: The input to this code would typically be a dataset that's used to train, validate and test your machine learning model. The format of the dataset can be .csv, .txt, .db etc. depending on the application. Although, .tfrecords is a common format used with TensorFlow models. 

Specifications: The dataset columns should be the features needed for the training the model and a column for the target variable or label which the model needs to predict. 

Do note the dataset should be cleaned and preprocessed before it is fed into the model for training. The features and targets need to be in numerical format as models cannot process categorical data or missing values.