Use Case: "prun" is used for running MPI processes across a cluster in a High-Performance Computing environment. This tool is part of the OpenMPI library. It allows one to manage parallel computations on distributed systems.

Code details and examples:

Code: 

```bash
#!/bin/bash
#SBATCH --job-name=test_mpi
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=8
#SBATCH --time=10:00
#SBATCH --mem=2000MB
#SBATCH --output=output.log

module load openmpi

prun ./my_program
```
Input File: `my_program` is the MPI executable file. It should be created from a source file using proper MPI commands.

Command to run: To submit the script to the scheduler, you would run the command `sbatch script.sh` where script.sh is the name of your script. 

The `sbatch` command submits the job to the SLURM scheduler. It accepts the `--job-name = test_mpi` parameter which assigns a name to the job for ease of administration and resource allocation. The node settings `--nodes = 2` and `--ntasks-per-node = 8` specify the number of nodes and the number of tasks per node. The `--time = 10:00` parameter sets the maximum time that the job can run and the `--mem = 2000MB` parameter sets the maximum amount of memory the job is allocated. The `--output = output.log` parameters specify where to write the job's standard output.
 
The third section of the script loads the openmpi module with `module load openmpi` and executes the MPI job on the cluster with `prun ./my_program`.

With SLURM, to specify the number of nodes and tasks, you use the #SBATCH --nodes, #SBATCH --ntasks-per-node directives. The prun command runs the MPI job on the nodes allocated by SLURM. 

Each node must have a copy of the MPI program executable `my_program`.
Before executing the program, `openmpi` module needs to be loaded, which is done using the module load command. The invocation of `prun` command does not require the input filename. 

Important note: You need to compile your MPI program with mpicc or a similar MPI-aware compiler before running `prun`.